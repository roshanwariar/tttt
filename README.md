# tttt
Tracing Tokens Through Transformers

Basically wrote some code to map the activations of a LLM into a PCA space to see how its position in "language" or "semantic" space over discrete (the layers of the network). Check out the blog for a full breakdown:https://roshanwariar.github.io/2025/12/16/t4/

Run t4.py for the plots and gif like the one below!

![Thought Evolution](/assets/images/thought_evolution.gif)
